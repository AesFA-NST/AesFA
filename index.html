<html><head lang="en"><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>AesFA</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="./index_files/icon.png">
    <link rel="stylesheet" href="./index_files/bootstrap.min.css">
    <link rel="stylesheet" href="./index_files/font-awesome.min.css">
    <link rel="stylesheet" href="./index_files/codemirror.min.css">
    <link rel="stylesheet" href="./index_files/app.css">
    <link rel="stylesheet" href="./index_files/bootstrap.min(1).css">

    <script type="text/javascript" async="" src="./index_files/analytics.js"></script>
    <script type="text/javascript" async="" src="./index_files/analytics(1).js"></script>
    <script async="" src="./index_files/js"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-110862391-3');
    </script>

    <script src="./index_files/jquery.min.js"></script>
    <script src="./index_files/bootstrap.min.js"></script>
    <script src="./index_files/codemirror.min.js"></script>
    <script src="./index_files/clipboard.min.js"></script>

    <script src="./index_files/app.js"></script>
</head>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DG5ZZGKNKT"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-DG5ZZGKNKT');
</script>        
    
<body data-gr-c-s-loaded="true">
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer
            <br /><br />
            <small>
                AAAI2024
            </small>
            <br /><br />
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <a href="https://ioahKwon.github.io">
                          Joonwoo Kwon
                        </a><sup>1*</sup>
                    </li>
                    <li>
                        <a href="https://sooyyoungg.github.io">
                          Sooyoung Kim
                        </a><sup>1*</sup>
                    </li>                    
                   <li>
                        <a href="https://ywlincq.github.io/">
                          Yuewei Lin
                        </a><sup>2†</sup>
                    </li>
                    <li>
                        <a href="http://sjyoo.com/">
                          Shinjae Yoo
                        </a><sup>2†</sup>
                    </li> 
                    <li>
                        <a href="https://www.connectomelab.com/home">
                          Jiook Cha
                        </a><sup>1†</sup>
                    </li>                      
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <sup>1</sup>Seoul National University
                    </li>
                    <br>
                    <li>
                        <sup>2</sup>Brookhaven National Laboratory
                    </li>
                    <br>
                    <li>
                        <sup>*</sup>Equal contribution
						<sup>†</sup>Co-corresponding authors
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2312.05928" target='_blank'>
                        <img src="./index_files/images/Paper.jpg" height="80px"><br>
                            <h4><strong>Main Paper</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="./supplementary.pdf" target='_blank'>
                        <img src="./index_files/images/Supplementary.jpg" height="80px"><br>
                            <h4><strong>Supp. Materials</strong></h4>
                        </a>
                    </li>
					<li>
                        <a href="./Rebuttal.pdf" target='_blank'>
                        <img src="./index_files/images/Rebuttal.jpg" height="80px"><br>
                            <h4><strong>Rebuttal</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#video">
                        <img src="index_files/images/youtube_icon_dark.png" height="80px"><br>
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
					<li>
                        <a href="https://github.com/Sooyyoungg/AesFA">
                        <img src="./index_files/images/github_pad.png" height="80px"><br>
                            <h4><strong>Code</strong></h4>
                        </a>
                    </li>
					<li>
                        <a href="#Colab Demo" target='_blank'>
                        <img src="./index_files/images/colab.png" height="80px"><br>
                            <h4><strong>Colab Demo</strong></h4>
                        </a>
                    </li>
                    <li>
                        <a href="#BibTeX">
                        <img src="./index_files/images/bibtex.jpg" height="80px"><br>
                            <h4><strong>BibTeX</strong></h4>
                        </a>
                    </li>
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Overview
                </h3>
				<img src="./index_files/images/Figure1.jpg" class="img-responsive" alt="overview1;style="display: block; margin: 0 auto;/><br>
				<img src="./index_files/images/Figure1_1.jpg" class="img-responsive" alt="overview2;style="display: block; margin: 0 auto;/><br>
				<p class="text-justify">
                    <ul>
                        <li>
                            Style representations are highly correlated to spatial information.
                        </li>
                        <li>
                            Existing methods heavily rely on pre-trained networks.
                        </li>
						
						<p></p>
			    	    <p>
						<b> &lt;Main Contributions&gt;</b>
						</p>
						
                        <li>
                            We propose a <b>lightweight</b> yet <b>effective</b> model for aesthetic feature extraction and stylization.
                        </li>
                        <li>
                            This framework introduces a new module for stylization and straightforward contrastive learning, while completely excluding pre-trained networks at inference.
                        </li>
                        <li>
                            Our approach achieves <b>SOTA</b> results <b>regardless of resolutions (256 to 4K)</b> while achieving faster inference.
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation 1 : Style representations are highly correlated to spatial information
                </h3>
                <p class="text-justify">
                   
                   Neural Style Transfer (NST) is an artistic application that transfers the style of one image to another while preserving the original content.
                   
                   Despite the recent advancement, a significant chasm persists between authentic artwork and synthesized stylizations.
                   
                   1. Existing NST methods struggle to capture essential aesthetic features, such as tones, brushstrokes, textures, grains and the local structure from style images, leading to discordant colors and irrelevant patterns. Ideally, the goal of using NST is to extract a style from the image and tansfer it to content, necessitating representations that capture both image semantics and stylistic changes. This work focuses on defining these <b>style</b> representations.
                   
                   
                   2. In the context of painting, style representations are dfined by attributes, such as overall color and/or the local structure of brushstrokes. Most NST algorithms define style representations as spatially agnostic features to encode this information, such as summary statistics. Thus, they lack spatial information representation.
                   
                   
                   3. In fact, style representations are highly correlated to spatial information. For example, Vincent van Gogh's <b>The Starry Night</b> has expressionistic yellow stars and a moon that dominate the upper center and right, while dynamic swirls fill the center of the sky. In pondering the style of this painting, its focal point primarily resides in the sky rather than the village or cypress tress. Therefore, when transferring <b>The Starry Night</b>'s style, the expected style output likely would be the dynamic swirls and expressionistic yellow stars in the sky. <b>From this point of view, spatial information keenly matters in style representations.</b>
                   
                   However, most NST algorithms fail to recognize such distinct spatial styles due to their spatial-independent style representations, leading to stylizations lacking in spatial coherence.
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Motivation 2: Are all negative samples necessary?
                </h3>
                <p class="text-justify">
                    Another challenge for NST is its heavy reliance on pre-trained networks, e.g., VGG (Simonyan and Zisereman 2014), for feature extraction. However, using such networks during inference is inefficient because of the computational demands from fully connected layers. This limits NST's use at high resolutions (e.g., 2K; 4K) and in mobile or real-time scenarios.
                    
                    To mitigate, a prior study (Wang et al. 2023) adopted contrastive learning for end-to-end training while excluding pretrained convolutional neural networks (CNNs) at inference. However, this approach is computationally expensive and inefficient as it uses all negative samples in a mini-batch, especially with higher-resolution samples.
                    
                    This prompts a question: <b>are all negative samples necessary?</b>
                    
                    Intuitively, the more distant negative samples contribute less to training as they are already well discriminated from the positive sample and vice versa.
                    
                
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Methods 1 : Overall Architecture
                </h3>
				<img src="./index_files/images/Figure2.jpg" class="img-responsive" alt="method1;style="display: block; margin: 0 auto;/><br>
                <p class="text-justify">
                    <ul>
                        <li>
                            To enhance stylization, we propose a lightweight yet effective model that we call, <b>Aes</b>thetic <b>F</b>eature-<b>A</b>ware Arbitrary NST, or <b>AesFA</b>.
                        </li>
                        <li>
                            AesFA overcomes prior NST limitations by encoding style representations while retaining spatial details.
                        </li>
                        <li>
                            To expedite the extraction of aesthetic features, we decompose the image into two discrete complementary components, i.e., the hihg- and low-frequency parts.
                        </li>
                        <li>
                            High frequency captures details including textures, grains, and brushstrokes, while low frequency encodes global structures and tones.
                        </li>
                    </ul>   
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Methods 2 : Aesthetic Feature-Aware Stylization
                </h3>
				<img src="./index_files/images/Figure3.jpg" class="img-responsive" alt="method2;style="display: block; margin: 0 auto;/><br>
                <p class="text-justify">
                    <ul>
                       <li>
                           To effectively infuse frequency-decomposed content features with aesthetic features, a new stylization module, AdaOct, is proposed that yields more satisfying stylizations with sophisticated aesthetic characteristics.
                       </li>
                       <li>
                          The active interactions between two frequencies that occur in OctConv could further enhance aesthetic stylization while reducing the total computational redundancy and unwated artifacts.
                       </li>
                    </ul>
                    
                     
                    
                      
                </p>
            </div>
        </div>
        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Methods 3 : Aesthetic Feature Contrastive Learning
                </h3>
				<img src="./index_files/images/Figure4.jpg" class="img-responsive" alt="method3;style="display: block; margin: 0 auto;/><br>
                <p class="text-justify">
                    <ul>
                     <li>
                         Inspired by hard negative mining, we redefine "negative" samples as the k-th nearest negative samples to the stylized output, introducing efficient contrastive learnign for aesthetic featues via pre-trained VGG network.
                     </li>
                    </ul>
            </div>
        </div>
		
		<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Implementation Details
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            To train our model, we use the COCO dataset as content images and the WikiArt dataset as style images.
                        </li>
                        <li>
                            All experiments were conducted using the PyTorch framework on a single NVIDIA A100 (40G) GPU.
                        </li>
                        <li>
                            We validated our model against eight state-of-the-art NST approaches. 
                        </li>
                        <li>
                            For fair comparisons, all existing algorithms are re-trained using same datasets with the respective author-released codes and default configurations. 
                        </li>
                        <li>
                            More details are outlined in the main paper and the supplementary materials.
                        </li>
                    </ul>
                </p>
            </div>
        </div>
		
		<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experimental Results
                </h3>
				<img src="./index_files/images/Figure5.jpg" class="img-responsive" alt="qualitative1;style="display: block; margin: 0 auto;/><br>
				<img src="./index_files/images/Figure6.jpg" class="img-responsive" alt="qualitative2;style="display: block; margin: 0 auto;/><br>
				<img src="./index_files/images/Figure7.jpg" class="img-responsive" alt="qualitative3;style="display: block; margin: 0 auto;/><br>
				<img src="./index_files/images/Figure8.jpg" class="img-responsive" alt="quantitative4;style="display: block; margin: 0 auto;/><br>
				<img src="./index_files/images/table.png" class="img-responsive" alt="quantitative5;style="display: block; margin: 0 auto;/><br>
                <p class="text-justify">
                <ul>
                    <li>
                        <b>AesFA outperforms</b> eight state-of-the-art NST techniques both <b>qualitatively and quantitatively</b> regardless of resolutions, especially in terms of aesthetics while maintaining the essential content semantics.
                    </li>
                    <li>
                        Compared to the other techniques, AesFA accomplishes the highest or at least comparable score along all evaluation metrics <b>regardless of image spatial resolution</b>, rendering a single image in less than 0.02 seconds.
                    </li>
                    <li>
                        More details including ablation studies are elaborated in the main paper.
                    </li>
                </ul>
                </p>
            </div>
        </div>
		
		<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Style Blending
                </h3>
                <img src="./index_files/images/Figure9.jpg" class="img-responsive" alt="method"><br>
                
                <p class="text-justify">
                   <ul>
                       <li>
                           Spatial control is also conveted by users who wish to modify an image by applying different styles to various regions of the image. We explored finer aesthetic style control by encoding style images into various frequencies, enabling users to blend content and style without additional resources.
                       </li>
                       
                       <li>
                           For example, Figure above shows the style blending, i.e., using the low-frequency and high-frequency style information from different images.
                       </li>
                       
                       <li>
                           The style-transferred outputs finely keep the color information from the low-frequency image and change the texture information based on the high frequency image.
                       </li>
                       
                       <li>
                           This could also be successfully adapted to the video style transfer and is well described in supplementary materials.
                       </li>             
                   </ul>
                </p>
            </div>
        </div>
		
		<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video Style Transfer
                </h3>
                
                <div style="width:370px; float:left;margin-right:10px;">
                    <video id="video1_id" width="100%" controls="" autoplay loop muted controlsList="nodownload">>
                            <source src="./index_files/videos/content.mp4" type="video/mp4">
                    </video>
                </div>
                
                <div style="width:370px; float:left;">
                    <video id="video2_id" width="100%" controls="" autoplay loop muted  controlsList="nodownload">>
                            <source src="./index_files/videos/stylized.mp4" type="video/mp4">
                    </video>
                    
                </div>
                <div style="clear:both:">
                   
                   <p class="text-justify">
                    &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<b>Left</b>: Content Video.&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp
                    <b>Right</b>: Stylized Output by AesFA in 2K (2048) resolution.
                    
                    </p>
                    <img src="./index_files/images/video_style.png" class="img-responsive" alt="videostyle"><br>
                    <p class="text-justify">
                        <ul>
                            <li>
                                AesFA achieves stylized output in less than 0.02 seconds; that is, our model processes 50 frames per second regardless of resolution and is suitable for real-time ultra-high resolution rendering (4K) applications.
                            </li>
                            <li>
                                Our results can yield the best video results in terms of high consistency and aesthetic features (e.g., colors and textures).
                            </li>
                        </ul>
                    </p>
                </div>
            </div>
        </div>
		
		<div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Limitations
                </h3>
                <img src="./index_files/images/supple_limit.png" class="img-responsive" alt="limit"><br>
                <p class="text-justify">
                    Despite its impressive performance, AesFA has ceratin limitations.
                    <ul>
                       <li>
                           The results by AesFA are sensitive to the weighting hyper-parameters for each loss, often resulting in over-stylized output (e.g., the repetitive intense style patterns on the backgrounds). This could be mitigated by carefully adjusting the weighting hyper-parameters.
                       </li>
                       <li>
                           The vertical line-shape artifacts alongside the images are often observed. We reason that these appear because the content features are being convolved directly with the predicted <b>aesthetic feature-aware kernels and biases</b> in our model. In addition, the upsampling operation could be the ones that create artifacts.
                       </li>
                    </ul>
                </p>
            </div>
        </div>
        
        
        
        <!--
        <div class="row" id="video">
            <div class="col-md-8 col-md-offset-2">
                <h3>Video</h3><p style="color:blue;font-size:11px;">
                <div class="text-center">
                    <video id="video1_id" width="100%" controls="" controlsList="nodownload">>
                        <source src="./index_files/videos/video.mp4" type="video/mp4">
                        <track label="English" kind="subtitles" srclang="en" src="./index_files/videos/captions.vtt">
                    </video>

                    <script type="text/javascript">
                        $(document).ready(function() {
                        var video = document.querySelector('#video_id'); // get the video element
                        var tracks = video.textTracks; // one for each track element
                        var track = tracks[0]; // corresponds to the first track element
                        track.mode = 'hidden';});
                    </script>

                </div>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Highlights
                </h3>
                <p class="text-justify">
                    <ul>
                        <li>
                            A human visual decoding system that only reply on limited annotations.
                        </li>
                        <li>
                            State-of-the-art 100-way top-1 classification accuracy on GOD dataset: <b>23.9%</b>, outperforming the previous best by <b>66%</b>.
                        </li>
                        <li>
                            State-of-the-art generation quality (FID) on GOD dataset: <b>1.67</b>, outperforming the previous best by <b>41%</b>.
                        </li>
                        <li>
                            For the first time, we show that non-invasive brain recordings can be used to decode images with similar performance as invasive measures.
                        </li>
                    </ul>
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    MinD-Vis
                </h3>
                <img src="./index_files/images/flowchart.png" class="img-responsive" alt="method"><br>
                <p class="text-justify">
                    <b>Stage A</b> (left): Self-supervised pre-training on large-scale fMRI dataset using Sparse-Coding based Masked Brain Modeling <b>(SC-MBM)</b>;
                    <b>Stage B</b> (right): Double-Conditioned Latent Diffusion Model <b>(DC-LDM)</b> for image generation conditioned on brain recordings.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results compared with benchmarks
                </h3>
                <img src="./index_files/images/compare_figs.png" class="img-responsive" alt="result with sota"><br>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Generation Consistency &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp Replication Dataset
                </h3>
                <img src="./index_files/images/more_result.png" class="img-responsive" alt="consistency and bold5000"><br>
            </div>
        </div>
-->

        <div class="row" id="BibTeX">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
               If you find our data or project useful in your research, please cite:
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 10px">
@article{kwon2023aesfa,
  title={AesFA: An Aesthetic Feature-Aware Arbitrary Neural Style Transfer},
  author={Kwon, Joonwoo and Kim, Sooyoung and Lin, Yuewei and Yoo, Shinjae and Cha, Jiook},
  journal={arXiv preprint arXiv:2312.05928},
  year={2023}
  }</pre>
            </div>
        </div>
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgments
                </h3>
                The website template was borrowed from <a href="https://mind-vis.github.io">MinD-Vis</a>.
                <p></p>
            </div>
        </div>
    </div>


</body></html>
